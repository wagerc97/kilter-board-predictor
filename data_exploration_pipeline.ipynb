{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kilter Board Data Exploration Pipeline\n",
    "\n",
    "This notebook provides a comprehensive data exploration pipeline for analyzing SQLite database containing climbing route data.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Database Connection](#setup)\n",
    "2. [Database Schema Exploration](#schema)\n",
    "3. [Data Quality Assessment](#quality)\n",
    "4. [Exploratory Data Analysis](#eda)\n",
    "5. [Statistical Analysis](#stats)\n",
    "6. [Visualization Pipeline](#viz)\n",
    "7. [Export and Reporting](#export)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Database Connection {#setup}\n",
    "\n",
    "First, let's import all necessary libraries and establish connection to our SQLite database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Visualization libraries\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Database handling\n",
    "from sqlalchemy import create_engine, text\n",
    "\n",
    "# Jupyter widgets for interactivity\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure pandas display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection setup\n",
    "DATABASE_PATH = 'kilter_board_data.db'\n",
    "\n",
    "def get_db_connection():\n",
    "    \"\"\"Create and return a database connection.\"\"\"\n",
    "    try:\n",
    "        conn = sqlite3.connect(DATABASE_PATH)\n",
    "        return conn\n",
    "    except Exception as e:\n",
    "        print(f\"Error connecting to database: {e}\")\n",
    "        return None\n",
    "\n",
    "def execute_query(query, params=None):\n",
    "    \"\"\"Execute a query and return results as a pandas DataFrame.\"\"\"\n",
    "    conn = get_db_connection()\n",
    "    if conn:\n",
    "        try:\n",
    "            df = pd.read_sql_query(query, conn, params=params)\n",
    "            conn.close()\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            print(f\"Error executing query: {e}\")\n",
    "            conn.close()\n",
    "            return None\n",
    "    return None\n",
    "\n",
    "# Test connection\n",
    "test_conn = get_db_connection()\n",
    "if test_conn:\n",
    "    print(\"‚úÖ Database connection successful!\")\n",
    "    test_conn.close()\n",
    "else:\n",
    "    print(\"‚ùå Database connection failed. Make sure to run 'python create_sample_db.py' first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Database Schema Exploration {#schema}\n",
    "\n",
    "Let's explore the database structure and understand our data model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of all tables\n",
    "tables_query = \"\"\"\n",
    "SELECT name FROM sqlite_master \n",
    "WHERE type='table' AND name NOT LIKE 'sqlite_%'\n",
    "ORDER BY name;\n",
    "\"\"\"\n",
    "\n",
    "tables_df = execute_query(tables_query)\n",
    "print(\"Database Tables:\")\n",
    "print(\"=\" * 20)\n",
    "for table in tables_df['name']:\n",
    "    print(f\"üìã {table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get schema information for each table\n",
    "def get_table_schema(table_name):\n",
    "    \"\"\"Get schema information for a specific table.\"\"\"\n",
    "    schema_query = f\"PRAGMA table_info({table_name})\"\n",
    "    return execute_query(schema_query)\n",
    "\n",
    "# Display schema for all tables\n",
    "for table in tables_df['name']:\n",
    "    print(f\"\\nüîç Schema for '{table}' table:\")\n",
    "    schema = get_table_schema(table)\n",
    "    print(schema.to_string(index=False))\n",
    "    \n",
    "    # Get row count\n",
    "    count_query = f\"SELECT COUNT(*) as row_count FROM {table}\"\n",
    "    count_result = execute_query(count_query)\n",
    "    print(f\"üìä Total rows: {count_result['row_count'].iloc[0]}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment {#quality}\n",
    "\n",
    "Let's assess the quality of our data by checking for missing values, duplicates, and data consistency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load all tables into DataFrames for analysis\n",
    "holds_df = execute_query(\"SELECT * FROM holds\")\n",
    "routes_df = execute_query(\"SELECT * FROM routes\")\n",
    "route_holds_df = execute_query(\"SELECT * FROM route_holds\")\n",
    "attempts_df = execute_query(\"SELECT * FROM user_attempts\")\n",
    "\n",
    "# Create a dictionary for easy access\n",
    "dataframes = {\n",
    "    'holds': holds_df,\n",
    "    'routes': routes_df,\n",
    "    'route_holds': route_holds_df,\n",
    "    'user_attempts': attempts_df\n",
    "}\n",
    "\n",
    "print(\"‚úÖ All tables loaded into DataFrames\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment function\n",
    "def assess_data_quality(df, table_name):\n",
    "    \"\"\"Assess data quality for a given DataFrame.\"\"\"\n",
    "    print(f\"\\nüìä Data Quality Report for '{table_name}' table:\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Basic info\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "    \n",
    "    # Missing values\n",
    "    missing_values = df.isnull().sum()\n",
    "    if missing_values.sum() > 0:\n",
    "        print(\"\\n‚ùå Missing Values:\")\n",
    "        for col, count in missing_values[missing_values > 0].items():\n",
    "            percentage = (count / len(df)) * 100\n",
    "            print(f\"  {col}: {count} ({percentage:.2f}%)\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No missing values found\")\n",
    "    \n",
    "    # Duplicate rows\n",
    "    duplicates = df.duplicated().sum()\n",
    "    if duplicates > 0:\n",
    "        print(f\"\\n‚ùå Duplicate rows: {duplicates}\")\n",
    "    else:\n",
    "        print(\"\\n‚úÖ No duplicate rows found\")\n",
    "    \n",
    "    # Data types\n",
    "    print(\"\\nüìã Data Types:\")\n",
    "    for col, dtype in df.dtypes.items():\n",
    "        print(f\"  {col}: {dtype}\")\n",
    "    \n",
    "    return {\n",
    "        'shape': df.shape,\n",
    "        'missing_values': missing_values.sum(),\n",
    "        'duplicates': duplicates,\n",
    "        'memory_usage_kb': df.memory_usage(deep=True).sum() / 1024\n",
    "    }\n",
    "\n",
    "# Assess quality for all tables\n",
    "quality_reports = {}\n",
    "for table_name, df in dataframes.items():\n",
    "    quality_reports[table_name] = assess_data_quality(df, table_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary quality dashboard\n",
    "quality_summary = pd.DataFrame(quality_reports).T\n",
    "quality_summary.columns = ['Shape', 'Missing Values', 'Duplicates', 'Memory (KB)']\n",
    "quality_summary['Rows'] = quality_summary['Shape'].apply(lambda x: x[0])\n",
    "quality_summary['Columns'] = quality_summary['Shape'].apply(lambda x: x[1])\n",
    "quality_summary = quality_summary[['Rows', 'Columns', 'Missing Values', 'Duplicates', 'Memory (KB)']]\n",
    "\n",
    "print(\"\\nüìã Data Quality Summary:\")\n",
    "print(\"=\" * 30)\n",
    "print(quality_summary.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis {#eda}\n",
    "\n",
    "Now let's dive into exploring our data with descriptive statistics and initial insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for numerical columns\n",
    "print(\"üìä Descriptive Statistics for Holds:\")\n",
    "print(\"=\" * 40)\n",
    "print(holds_df.describe())\n",
    "\n",
    "print(\"\\nüìä Descriptive Statistics for Routes:\")\n",
    "print(\"=\" * 40)\n",
    "print(routes_df.describe())\n",
    "\n",
    "print(\"\\nüìä Descriptive Statistics for User Attempts:\")\n",
    "print(\"=\" * 40)\n",
    "print(attempts_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Categorical analysis\n",
    "print(\"üè∑Ô∏è Hold Types Distribution:\")\n",
    "hold_types = holds_df['hold_type'].value_counts()\n",
    "print(hold_types)\n",
    "\n",
    "print(\"\\nüéØ Grade Distribution:\")\n",
    "grade_dist = routes_df['grade'].value_counts().sort_index()\n",
    "print(grade_dist)\n",
    "\n",
    "print(\"\\nüë§ Setter Distribution:\")\n",
    "setter_dist = routes_df['setter_name'].value_counts()\n",
    "print(setter_dist)\n",
    "\n",
    "print(\"\\nüé™ Attempt Results Distribution:\")\n",
    "attempt_results = attempts_df['attempt_result'].value_counts()\n",
    "print(attempt_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced queries for insights\n",
    "insights_queries = {\n",
    "    'route_difficulty_by_setter': \"\"\"\n",
    "        SELECT setter_name, AVG(grade_numeric) as avg_difficulty, COUNT(*) as routes_set\n",
    "        FROM routes \n",
    "        GROUP BY setter_name \n",
    "        ORDER BY avg_difficulty DESC\n",
    "    \"\"\",\n",
    "    \n",
    "    'success_rate_by_grade': \"\"\"\n",
    "        SELECT r.grade, \n",
    "               COUNT(CASE WHEN ua.attempt_result IN ('flash', 'send') THEN 1 END) as successful_attempts,\n",
    "               COUNT(*) as total_attempts,\n",
    "               ROUND(COUNT(CASE WHEN ua.attempt_result IN ('flash', 'send') THEN 1 END) * 100.0 / COUNT(*), 2) as success_rate\n",
    "        FROM user_attempts ua\n",
    "        JOIN routes r ON ua.route_id = r.id\n",
    "        GROUP BY r.grade\n",
    "        ORDER BY r.grade_numeric\n",
    "    \"\"\",\n",
    "    \n",
    "    'most_popular_routes': \"\"\"\n",
    "        SELECT r.name, r.grade, COUNT(*) as attempt_count,\n",
    "               ROUND(AVG(ua.difficulty_rating), 2) as avg_user_rating\n",
    "        FROM user_attempts ua\n",
    "        JOIN routes r ON ua.route_id = r.id\n",
    "        GROUP BY r.id, r.name, r.grade\n",
    "        ORDER BY attempt_count DESC\n",
    "        LIMIT 10\n",
    "    \"\"\",\n",
    "    \n",
    "    'hold_usage_stats': \"\"\"\n",
    "        SELECT h.hold_type, COUNT(*) as usage_count, AVG(h.difficulty_contribution) as avg_difficulty\n",
    "        FROM holds h\n",
    "        JOIN route_holds rh ON h.id = rh.hold_id\n",
    "        GROUP BY h.hold_type\n",
    "        ORDER BY usage_count DESC\n",
    "    \"\"\"\n",
    "}\n",
    "\n",
    "# Execute insight queries\n",
    "insights = {}\n",
    "for query_name, query in insights_queries.items():\n",
    "    result = execute_query(query)\n",
    "    insights[query_name] = result\n",
    "    print(f\"\\nüîç {query_name.replace('_', ' ').title()}:\")\n",
    "    print(\"=\" * 50)\n",
    "    print(result.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Statistical Analysis {#stats}\n",
    "\n",
    "Let's perform some statistical analysis to understand relationships and correlations in our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "numerical_cols = ['grade_numeric', 'difficulty_rating', 'attempts_count']\n",
    "\n",
    "# Create a comprehensive dataset for correlation analysis\n",
    "analysis_df = execute_query(\"\"\"\n",
    "    SELECT \n",
    "        r.grade_numeric,\n",
    "        ua.difficulty_rating,\n",
    "        ua.attempts_count,\n",
    "        CASE WHEN ua.attempt_result IN ('flash', 'send') THEN 1 ELSE 0 END as success,\n",
    "        ua.user_id,\n",
    "        r.id as route_id\n",
    "    FROM user_attempts ua\n",
    "    JOIN routes r ON ua.route_id = r.id\n",
    "\"\"\")\n",
    "\n",
    "print(\"üìä Correlation Matrix:\")\n",
    "correlation_matrix = analysis_df[numerical_cols].corr()\n",
    "print(correlation_matrix)\n",
    "\n",
    "# Statistical tests\n",
    "from scipy import stats\n",
    "\n",
    "print(\"\\nüßÆ Statistical Tests:\")\n",
    "print(\"=\" * 30)\n",
    "\n",
    "# Test correlation between grade and user difficulty rating\n",
    "correlation, p_value = stats.pearsonr(analysis_df['grade_numeric'], analysis_df['difficulty_rating'])\n",
    "print(f\"Grade vs User Rating Correlation: {correlation:.3f} (p-value: {p_value:.3f})\")\n",
    "\n",
    "# Test correlation between grade and attempts needed\n",
    "correlation, p_value = stats.pearsonr(analysis_df['grade_numeric'], analysis_df['attempts_count'])\n",
    "print(f\"Grade vs Attempts Correlation: {correlation:.3f} (p-value: {p_value:.3f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# User performance analysis\n",
    "user_stats = execute_query(\"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        COUNT(*) as total_attempts,\n",
    "        COUNT(CASE WHEN attempt_result IN ('flash', 'send') THEN 1 END) as successful_attempts,\n",
    "        ROUND(COUNT(CASE WHEN attempt_result IN ('flash', 'send') THEN 1 END) * 100.0 / COUNT(*), 2) as success_rate,\n",
    "        AVG(difficulty_rating) as avg_perceived_difficulty,\n",
    "        MAX(r.grade_numeric) as hardest_grade_attempted,\n",
    "        AVG(attempts_count) as avg_attempts_per_route\n",
    "    FROM user_attempts ua\n",
    "    JOIN routes r ON ua.route_id = r.id\n",
    "    GROUP BY user_id\n",
    "    HAVING total_attempts >= 10\n",
    "    ORDER BY success_rate DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"üë• User Performance Statistics:\")\n",
    "print(\"=\" * 40)\n",
    "print(user_stats.head(10).to_string(index=False))\n",
    "\n",
    "print(f\"\\nüìà Performance Summary:\")\n",
    "print(f\"Average Success Rate: {user_stats['success_rate'].mean():.2f}%\")\n",
    "print(f\"Median Success Rate: {user_stats['success_rate'].median():.2f}%\")\n",
    "print(f\"Best Performer: User {user_stats.iloc[0]['user_id']} ({user_stats.iloc[0]['success_rate']:.2f}% success rate)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Visualization Pipeline {#viz}\n",
    "\n",
    "Now let's create comprehensive visualizations to better understand our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the plotting environment\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "fig.suptitle('Kilter Board Data Overview', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Plot 1: Hold types distribution\n",
    "hold_types.plot(kind='bar', ax=axes[0,0], color='skyblue')\n",
    "axes[0,0].set_title('Distribution of Hold Types')\n",
    "axes[0,0].set_xlabel('Hold Type')\n",
    "axes[0,0].set_ylabel('Count')\n",
    "axes[0,0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 2: Grade distribution\n",
    "grade_dist.plot(kind='bar', ax=axes[0,1], color='lightcoral')\n",
    "axes[0,1].set_title('Distribution of Route Grades')\n",
    "axes[0,1].set_xlabel('Grade')\n",
    "axes[0,1].set_ylabel('Count')\n",
    "axes[0,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Plot 3: Attempt results\n",
    "attempt_results.plot(kind='pie', ax=axes[1,0], autopct='%1.1f%%')\n",
    "axes[1,0].set_title('Attempt Results Distribution')\n",
    "axes[1,0].set_ylabel('')\n",
    "\n",
    "# Plot 4: Success rate by grade\n",
    "success_by_grade = insights['success_rate_by_grade']\n",
    "axes[1,1].bar(success_by_grade['grade'], success_by_grade['success_rate'], color='lightgreen')\n",
    "axes[1,1].set_title('Success Rate by Grade')\n",
    "axes[1,1].set_xlabel('Grade')\n",
    "axes[1,1].set_ylabel('Success Rate (%)')\n",
    "axes[1,1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Plotly visualizations\n",
    "# 1. Hold positions on the board\n",
    "fig_holds = px.scatter(holds_df, x='position_x', y='position_y', \n",
    "                      color='hold_type', size='difficulty_contribution',\n",
    "                      title='Hold Positions on Kilter Board',\n",
    "                      labels={'position_x': 'X Position (%)', 'position_y': 'Y Position (%)'},\n",
    "                      width=800, height=600)\n",
    "fig_holds.update_layout(showlegend=True)\n",
    "fig_holds.show()\n",
    "\n",
    "# 2. User performance heatmap\n",
    "user_grade_performance = execute_query(\"\"\"\n",
    "    SELECT \n",
    "        ua.user_id,\n",
    "        r.grade,\n",
    "        COUNT(CASE WHEN ua.attempt_result IN ('flash', 'send') THEN 1 END) as successes,\n",
    "        COUNT(*) as attempts\n",
    "    FROM user_attempts ua\n",
    "    JOIN routes r ON ua.route_id = r.id\n",
    "    GROUP BY ua.user_id, r.grade\n",
    "    HAVING attempts >= 3\n",
    "\"\"\")\n",
    "user_grade_performance['success_rate'] = user_grade_performance['successes'] / user_grade_performance['attempts']\n",
    "\n",
    "# Create pivot table for heatmap\n",
    "heatmap_data = user_grade_performance.pivot(index='user_id', columns='grade', values='success_rate')\n",
    "heatmap_data = heatmap_data.fillna(0)\n",
    "\n",
    "fig_heatmap = px.imshow(heatmap_data, \n",
    "                       title='User Success Rate by Grade',\n",
    "                       labels={'x': 'Grade', 'y': 'User ID', 'color': 'Success Rate'},\n",
    "                       aspect='auto',\n",
    "                       color_continuous_scale='RdYlGn')\n",
    "fig_heatmap.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced time series analysis\n",
    "# Convert timestamp to datetime\n",
    "attempts_df['attempted_at'] = pd.to_datetime(attempts_df['attempted_at'])\n",
    "attempts_df['date'] = attempts_df['attempted_at'].dt.date\n",
    "attempts_df['month'] = attempts_df['attempted_at'].dt.to_period('M')\n",
    "\n",
    "# Daily activity\n",
    "daily_activity = attempts_df.groupby('date').size().reset_index(name='attempts')\n",
    "daily_activity['date'] = pd.to_datetime(daily_activity['date'])\n",
    "\n",
    "fig_time = px.line(daily_activity, x='date', y='attempts',\n",
    "                  title='Daily Climbing Activity Over Time',\n",
    "                  labels={'date': 'Date', 'attempts': 'Number of Attempts'})\n",
    "fig_time.show()\n",
    "\n",
    "# Monthly success rate trends\n",
    "monthly_stats = attempts_df.groupby('month').agg({\n",
    "    'attempt_result': lambda x: (x.isin(['flash', 'send'])).sum(),\n",
    "    'id': 'count'\n",
    "}).reset_index()\n",
    "monthly_stats.columns = ['month', 'successes', 'total_attempts']\n",
    "monthly_stats['success_rate'] = monthly_stats['successes'] / monthly_stats['total_attempts']\n",
    "monthly_stats['month_str'] = monthly_stats['month'].astype(str)\n",
    "\n",
    "fig_monthly = px.bar(monthly_stats, x='month_str', y='success_rate',\n",
    "                    title='Monthly Success Rate Trends',\n",
    "                    labels={'month_str': 'Month', 'success_rate': 'Success Rate'})\n",
    "fig_monthly.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive dashboard with widgets\n",
    "def create_interactive_analysis():\n",
    "    \"\"\"Create interactive widgets for data exploration.\"\"\"\n",
    "    \n",
    "    # Widget for grade selection\n",
    "    grade_widget = widgets.SelectMultiple(\n",
    "        options=list(routes_df['grade'].unique()),\n",
    "        value=list(routes_df['grade'].unique())[:3],\n",
    "        description='Grades:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    # Widget for hold type selection\n",
    "    hold_widget = widgets.SelectMultiple(\n",
    "        options=list(holds_df['hold_type'].unique()),\n",
    "        value=list(holds_df['hold_type'].unique()),\n",
    "        description='Hold Types:',\n",
    "        disabled=False\n",
    "    )\n",
    "    \n",
    "    def update_analysis(selected_grades, selected_holds):\n",
    "        \"\"\"Update analysis based on widget selections.\"\"\"\n",
    "        filtered_query = f\"\"\"\n",
    "            SELECT r.grade, COUNT(*) as route_count, AVG(ua.difficulty_rating) as avg_rating\n",
    "            FROM routes r\n",
    "            LEFT JOIN user_attempts ua ON r.id = ua.route_id\n",
    "            WHERE r.grade IN ({','.join([f\"'{g}'\" for g in selected_grades])})\n",
    "            GROUP BY r.grade\n",
    "            ORDER BY r.grade_numeric\n",
    "        \"\"\"\n",
    "        \n",
    "        result = execute_query(filtered_query)\n",
    "        \n",
    "        if result is not None and not result.empty:\n",
    "            fig = px.bar(result, x='grade', y='route_count',\n",
    "                        title=f'Route Count for Selected Grades',\n",
    "                        color='avg_rating')\n",
    "            fig.show()\n",
    "    \n",
    "    # Create interactive widget\n",
    "    interactive_plot = widgets.interactive(update_analysis, \n",
    "                                         selected_grades=grade_widget,\n",
    "                                         selected_holds=hold_widget)\n",
    "    \n",
    "    display(interactive_plot)\n",
    "\n",
    "# Create the interactive dashboard\n",
    "create_interactive_analysis()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Export and Reporting {#export}\n",
    "\n",
    "Finally, let's create export capabilities and generate summary reports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export functions\n",
    "def export_summary_report():\n",
    "    \"\"\"Generate and export a comprehensive summary report.\"\"\"\n",
    "    \n",
    "    report = {\n",
    "        'database_overview': {\n",
    "            'total_holds': len(holds_df),\n",
    "            'total_routes': len(routes_df),\n",
    "            'total_attempts': len(attempts_df),\n",
    "            'unique_users': attempts_df['user_id'].nunique(),\n",
    "            'date_range': f\"{attempts_df['attempted_at'].min()} to {attempts_df['attempted_at'].max()}\"\n",
    "        },\n",
    "        'route_statistics': {\n",
    "            'grade_distribution': routes_df['grade'].value_counts().to_dict(),\n",
    "            'setter_distribution': routes_df['setter_name'].value_counts().to_dict(),\n",
    "            'average_difficulty': routes_df['grade_numeric'].mean()\n",
    "        },\n",
    "        'performance_metrics': {\n",
    "            'overall_success_rate': len(attempts_df[attempts_df['attempt_result'].isin(['flash', 'send'])]) / len(attempts_df) * 100,\n",
    "            'average_attempts_per_route': attempts_df['attempts_count'].mean(),\n",
    "            'most_difficult_grade': routes_df.loc[routes_df['grade_numeric'].idxmax(), 'grade'],\n",
    "            'most_popular_route': insights['most_popular_routes'].iloc[0]['name']\n",
    "        },\n",
    "        'data_quality': quality_reports\n",
    "    }\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Generate the report\n",
    "summary_report = export_summary_report()\n",
    "\n",
    "print(\"üìã KILTER BOARD DATA ANALYSIS SUMMARY REPORT\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "print(\"\\nüóÑÔ∏è Database Overview:\")\n",
    "for key, value in summary_report['database_overview'].items():\n",
    "    print(f\"  {key.replace('_', ' ').title()}: {value}\")\n",
    "\n",
    "print(\"\\nüìä Route Statistics:\")\n",
    "print(f\"  Average Difficulty: {summary_report['route_statistics']['average_difficulty']:.2f}\")\n",
    "print(f\"  Most Common Grade: {max(summary_report['route_statistics']['grade_distribution'], key=summary_report['route_statistics']['grade_distribution'].get)}\")\n",
    "print(f\"  Most Active Setter: {max(summary_report['route_statistics']['setter_distribution'], key=summary_report['route_statistics']['setter_distribution'].get)}\")\n",
    "\n",
    "print(\"\\nüéØ Performance Metrics:\")\n",
    "for key, value in summary_report['performance_metrics'].items():\n",
    "    if isinstance(value, float):\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value:.2f}\")\n",
    "    else:\n",
    "        print(f\"  {key.replace('_', ' ').title()}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to CSV files\n",
    "def export_to_csv():\n",
    "    \"\"\"Export processed data to CSV files for further analysis.\"\"\"\n",
    "    \n",
    "    # Create exports directory\n",
    "    import os\n",
    "    os.makedirs('exports', exist_ok=True)\n",
    "    \n",
    "    # Export main datasets\n",
    "    holds_df.to_csv('exports/holds_data.csv', index=False)\n",
    "    routes_df.to_csv('exports/routes_data.csv', index=False)\n",
    "    attempts_df.to_csv('exports/attempts_data.csv', index=False)\n",
    "    \n",
    "    # Export analysis results\n",
    "    user_stats.to_csv('exports/user_performance_stats.csv', index=False)\n",
    "    \n",
    "    for name, df in insights.items():\n",
    "        df.to_csv(f'exports/{name}.csv', index=False)\n",
    "    \n",
    "    print(\"‚úÖ Data exported to CSV files in 'exports/' directory\")\n",
    "    print(\"Files created:\")\n",
    "    for file in os.listdir('exports'):\n",
    "        print(f\"  üìÑ {file}\")\n",
    "\n",
    "# Export the data\n",
    "export_to_csv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save report as JSON for programmatic access\n",
    "import json\n",
    "\n",
    "with open('exports/analysis_report.json', 'w') as f:\n",
    "    json.dump(summary_report, f, indent=2, default=str)\n",
    "\n",
    "print(\"‚úÖ Complete analysis report saved as 'exports/analysis_report.json'\")\n",
    "\n",
    "# Display final summary\n",
    "print(\"\\nüéâ DATA EXPLORATION PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\" * 55)\n",
    "print(\"‚úÖ Database connection established and tested\")\n",
    "print(\"‚úÖ Schema exploration completed\")\n",
    "print(\"‚úÖ Data quality assessment performed\")\n",
    "print(\"‚úÖ Exploratory data analysis conducted\")\n",
    "print(\"‚úÖ Statistical analysis completed\")\n",
    "print(\"‚úÖ Comprehensive visualizations created\")\n",
    "print(\"‚úÖ Data exported for further analysis\")\n",
    "print(\"‚úÖ Summary report generated\")\n",
    "print(\"\\nüìÅ All results saved in 'exports/' directory\")\n",
    "print(\"\\nüî¨ Ready for advanced analysis and modeling!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}