{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8c5a2a1-8932-4b3e-a5fb-1c01e5b4b1f9",
   "metadata": {},
   "source": [
    "# Kilter Board Predictor - Data Exploration\n",
    "\n",
    "This notebook demonstrates the data exploration and basic statistics capabilities of the Kilter Board Predictor project. It shows how to analyze climbing route data to prepare for machine learning model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c3b5f5-2b4a-4b6e-8a41-c3b0e1e7d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from kilter_board_predictor import DataExplorer, BasicStatistics\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9b3a2d4-5c5e-4a5e-8b1a-d8c6e8d5f6e0",
   "metadata": {},
   "source": [
    "## 1. Create Sample Climbing Route Data\n",
    "\n",
    "Since we don't have real data yet, let's create a sample dataset that represents typical climbing route data with features that might be relevant for predicting route difficulty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4c1f5e6-1a2a-4e5e-8b77-a9b8c2a5c1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Create sample climbing route data\n",
    "n_routes = 1000\n",
    "\n",
    "# Generate synthetic climbing route data\n",
    "data = {\n",
    "    'route_id': range(1, n_routes + 1),\n",
    "    'angle': np.random.choice([0, 20, 30, 40, 50, 60, 70], n_routes, p=[0.1, 0.15, 0.2, 0.25, 0.15, 0.1, 0.05]),\n",
    "    'num_holds': np.random.randint(8, 25, n_routes),\n",
    "    'height': np.random.normal(3.5, 0.8, n_routes),  # Height in meters\n",
    "    'wall_type': np.random.choice(['vertical', 'overhang', 'slab'], n_routes, p=[0.4, 0.5, 0.1]),\n",
    "    'hold_type': np.random.choice(['crimps', 'jugs', 'slopers', 'pinches', 'mixed'], n_routes, p=[0.3, 0.2, 0.2, 0.1, 0.2]),\n",
    "    'setter_experience': np.random.randint(1, 10, n_routes),  # Years of experience\n",
    "    'completion_rate': np.random.beta(2, 3, n_routes),  # Success rate (0-1)\n",
    "    'average_attempts': np.random.poisson(3, n_routes) + 1,\n",
    "    'grade_v_scale': np.random.randint(0, 17, n_routes),  # V0 to V16\n",
    "}\n",
    "\n",
    "# Create correlations to make data more realistic\n",
    "# Higher angles tend to have higher grades\n",
    "angle_bonus = (data['angle'] / 70) * 3\n",
    "data['grade_v_scale'] = np.clip(data['grade_v_scale'] + angle_bonus, 0, 16).astype(int)\n",
    "\n",
    "# More holds generally means easier routes\n",
    "hold_penalty = (data['num_holds'] - 15) * -0.2\n",
    "data['grade_v_scale'] = np.clip(data['grade_v_scale'] + hold_penalty, 0, 16).astype(int)\n",
    "\n",
    "# Adjust completion rate based on grade\n",
    "grade_difficulty = data['grade_v_scale'] / 16\n",
    "data['completion_rate'] = np.clip(1 - grade_difficulty * 0.7 + np.random.normal(0, 0.1, n_routes), 0.05, 0.95)\n",
    "\n",
    "# Create DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Add some missing values to make it realistic\n",
    "missing_indices = np.random.choice(df.index, size=int(0.05 * len(df)), replace=False)\n",
    "df.loc[missing_indices, 'completion_rate'] = np.nan\n",
    "\n",
    "missing_indices_2 = np.random.choice(df.index, size=int(0.02 * len(df)), replace=False)\n",
    "df.loc[missing_indices_2, 'height'] = np.nan\n",
    "\n",
    "print(f\"Created sample dataset with {len(df)} routes\")\n",
    "print(f\"Shape: {df.shape}\")\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2b3c5e7-4e5d-4a5f-8b8c-e7f8d9e2a1a7",
   "metadata": {},
   "source": [
    "## 2. Data Exploration with DataExplorer Class\n",
    "\n",
    "Let's use our DataExplorer class to analyze the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a7b9c5-2e5f-4a8b-9c5a-1d2e3a4b5c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the DataExplorer\n",
    "explorer = DataExplorer(df)\n",
    "\n",
    "# Get comprehensive data overview\n",
    "overview = explorer.data_overview()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e4d6c7-8e5f-4a6b-9c2d-3e4f5a6b7c8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze missing values\n",
    "missing_analysis = explorer.missing_values_analysis(visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "g4h5i6j7-k8l9-m0n1-o2p3-q4r5s6t7u8v9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot distributions of numeric variables\n",
    "print(\"=== Distribution Analysis ===\")\n",
    "explorer.plot_distributions(plot_type='histogram')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "h5i6j7k8-l9m0-n1o2-p3q4-r5s6t7u8v9w0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create box plots for outlier detection\n",
    "print(\"=== Box Plots for Outlier Detection ===\")\n",
    "explorer.plot_distributions(plot_type='box')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "i6j7k8l9-m0n1-o2p3-q4r5-s6t7u8v9w0x1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlations\n",
    "print(\"=== Correlation Analysis ===\")\n",
    "corr_matrix = explorer.correlation_analysis(method='pearson', visualize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "j7k8l9m0-n1o2-p3q4-r5s6-t7u8v9w0x1y2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot categorical distributions\n",
    "print(\"=== Categorical Variable Distributions ===\")\n",
    "explorer.plot_categorical_distributions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "k8l9m0n1-o2p3-q4r5-s6t7-u8v9w0x1y2z3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detect outliers\n",
    "print(\"=== Outlier Detection ===\")\n",
    "outliers = explorer.outlier_detection(method='iqr', visualize=True)\n",
    "\n",
    "# Print outlier summary\n",
    "for col, outlier_data in outliers.items():\n",
    "    if len(outlier_data) > 0:\n",
    "        print(f\"{col}: {len(outlier_data)} outliers detected\")\n",
    "    else:\n",
    "        print(f\"{col}: No outliers detected\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "l9m0n1o2-p3q4-r5s6-t7u8-v9w0x1y2z3a4",
   "metadata": {},
   "source": [
    "## 3. Statistical Analysis with BasicStatistics Class\n",
    "\n",
    "Now let's perform comprehensive statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "m0n1o2p3-q4r5-s6t7-u8v9-w0x1y2z3a4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the BasicStatistics analyzer\n",
    "stats_analyzer = BasicStatistics(df)\n",
    "\n",
    "# Get descriptive statistics\n",
    "print(\"=== Descriptive Statistics ===\")\n",
    "desc_stats = stats_analyzer.descriptive_statistics()\n",
    "\n",
    "# Display key statistics for each numeric column\n",
    "for col, stats in desc_stats.items():\n",
    "    if 'error' not in stats:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Mean: {stats['mean']:.3f}, Median: {stats['median']:.3f}\")\n",
    "        print(f\"  Std: {stats['std']:.3f}, Range: {stats['range']:.3f}\")\n",
    "        print(f\"  Skewness: {stats['skewness']:.3f}, Kurtosis: {stats['kurtosis']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n1o2p3q4-r5s6-t7u8-v9w0-x1y2z3a4b5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for normality\n",
    "print(\"=== Normality Tests ===\")\n",
    "normality_results = stats_analyzer.normality_tests()\n",
    "\n",
    "for col, results in normality_results.items():\n",
    "    if 'error' not in results:\n",
    "        print(f\"\\n{col}:\")\n",
    "        if isinstance(results['shapiro_wilk']['is_normal'], bool):\n",
    "            print(f\"  Shapiro-Wilk: p={results['shapiro_wilk']['p_value']:.6f}, Normal: {results['shapiro_wilk']['is_normal']}\")\n",
    "        print(f\"  Kolmogorov-Smirnov: p={results['kolmogorov_smirnov']['p_value']:.6f}, Normal: {results['kolmogorov_smirnov']['is_normal']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "o2p3q4r5-s6t7-u8v9-w0x1-y2z3a4b5c6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze correlations with statistical significance\n",
    "print(\"=== Statistical Correlation Analysis ===\")\n",
    "corr_stats = stats_analyzer.correlation_statistics()\n",
    "\n",
    "print(f\"Mean absolute correlation: {corr_stats['summary']['mean_absolute_correlation']:.4f}\")\n",
    "print(f\"Number of strong correlations (|r| >= 0.7): {corr_stats['summary']['num_strong_pairs']}\")\n",
    "print(f\"Number of high correlations (0.5 <= |r| < 0.7): {corr_stats['summary']['num_high_pairs']}\")\n",
    "\n",
    "if corr_stats['strong_correlations']:\n",
    "    print(\"\\nStrong correlations:\")\n",
    "    for var1, var2, corr in corr_stats['strong_correlations']:\n",
    "        print(f\"  {var1} - {var2}: {corr:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "p3q4r5s6-t7u8-v9w0-x1y2-z3a4b5c6d7e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze categorical variables\n",
    "print(\"=== Categorical Statistics ===\")\n",
    "cat_stats = stats_analyzer.categorical_statistics()\n",
    "\n",
    "for col, stats in cat_stats.items():\n",
    "    if 'error' not in stats:\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Unique values: {stats['unique_values']}\")\n",
    "        print(f\"  Mode: {stats['mode']} ({stats['mode_percentage']:.1f}%)\")\n",
    "        print(f\"  Entropy: {stats['entropy']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "q4r5s6t7-u8v9-w0x1-y2z3-a4b5c6d7e8f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform hypothesis tests with grade as target variable\n",
    "print(\"=== Hypothesis Tests (Target: grade_v_scale) ===\")\n",
    "hypothesis_results = stats_analyzer.hypothesis_tests('grade_v_scale')\n",
    "\n",
    "# Sort by p-value to see most significant relationships\n",
    "significant_features = []\n",
    "for feature, result in hypothesis_results.items():\n",
    "    if 'error' not in result:\n",
    "        significant_features.append((feature, result['p_value'], result['test_name'], result['significant']))\n",
    "\n",
    "significant_features.sort(key=lambda x: x[1])  # Sort by p-value\n",
    "\n",
    "print(\"\\nFeature significance (sorted by p-value):\")\n",
    "for feature, p_value, test_name, significant in significant_features:\n",
    "    significance = \"***\" if significant else \"   \"\n",
    "    print(f\"  {significance} {feature}: p={p_value:.6f} ({test_name})\")\n",
    "\n",
    "print(\"\\n*** indicates p < 0.05 (statistically significant)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "r5s6t7u8-v9w0-x1y2-z3a4-b5c6d7e8f9g0",
   "metadata": {},
   "source": [
    "## 4. Generate Comprehensive Reports\n",
    "\n",
    "Let's generate comprehensive reports for both data exploration and statistical analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "s6t7u8v9-w0x1-y2z3-a4b5-c6d7e8f9g0h1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate data exploration report\n",
    "print(\"=== DATA EXPLORATION REPORT ===\")\n",
    "exploration_report = explorer.generate_report()\n",
    "print(exploration_report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "t7u8v9w0-x1y2-z3a4-b5c6-d7e8f9g0h1i2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate statistical analysis report\n",
    "print(\"\\n=== STATISTICAL ANALYSIS REPORT ===\")\n",
    "stats_report = stats_analyzer.generate_statistics_report(target_column='grade_v_scale')\n",
    "print(stats_report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "u8v9w0x1-y2z3-a4b5-c6d7-e8f9g0h1i2j3",
   "metadata": {},
   "source": [
    "## 5. Key Insights for Machine Learning\n",
    "\n",
    "Based on our analysis, here are key insights that will inform our machine learning approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "v9w0x1y2-z3a4-b5c6-d7e8-f9g0h1i2j3k4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== KEY INSIGHTS FOR MACHINE LEARNING ===\")\n",
    "print(\"\\n1. DATA QUALITY:\")\n",
    "missing_summary = explorer.missing_values_analysis(visualize=False)\n",
    "total_missing = missing_summary['Missing_Count'].sum()\n",
    "print(f\"   - Total missing values: {total_missing} ({(total_missing/(len(df)*len(df.columns)))*100:.2f}% of dataset)\")\n",
    "print(f\"   - No major data quality issues detected\")\n",
    "\n",
    "print(\"\\n2. FEATURE RELATIONSHIPS:\")\n",
    "corr_stats = stats_analyzer.correlation_statistics()\n",
    "print(f\"   - {corr_stats['summary']['num_strong_pairs']} strong correlations (|r| >= 0.7) found\")\n",
    "print(f\"   - {corr_stats['summary']['num_high_pairs']} moderate correlations (0.5 <= |r| < 0.7) found\")\n",
    "if corr_stats['strong_correlations']:\n",
    "    print(\"   - Watch for multicollinearity in these pairs:\")\n",
    "    for var1, var2, corr in corr_stats['strong_correlations'][:3]:\n",
    "        print(f\"     * {var1} - {var2}: {corr:.3f}\")\n",
    "\n",
    "print(\"\\n3. TARGET VARIABLE (grade_v_scale):\")\n",
    "grade_stats = desc_stats['grade_v_scale']\n",
    "print(f\"   - Range: V{int(grade_stats['min'])} to V{int(grade_stats['max'])}\")\n",
    "print(f\"   - Distribution: Mean={grade_stats['mean']:.1f}, Median={grade_stats['median']:.1f}\")\n",
    "print(f\"   - Skewness: {grade_stats['skewness']:.3f} ({'right-skewed' if grade_stats['skewness'] > 0.5 else 'approximately symmetric' if abs(grade_stats['skewness']) <= 0.5 else 'left-skewed'})\")\n",
    "\n",
    "print(\"\\n4. MOST PREDICTIVE FEATURES:\")\n",
    "significant_features = [(f, r['p_value']) for f, r in hypothesis_results.items() \n",
    "                       if 'error' not in r and r['significant']]\n",
    "significant_features.sort(key=lambda x: x[1])\n",
    "print(f\"   - {len(significant_features)} features show significant relationship with grade\")\n",
    "for feature, p_val in significant_features[:5]:  # Top 5\n",
    "    print(f\"     * {feature}: p={p_val:.6f}\")\n",
    "\n",
    "print(\"\\n5. RECOMMENDATIONS FOR MODEL TRAINING:\")\n",
    "print(\"   - Consider both regression and classification approaches\")\n",
    "print(\"   - Handle missing values (imputation or removal)\")\n",
    "if corr_stats['summary']['num_strong_pairs'] > 0:\n",
    "    print(\"   - Address multicollinearity (PCA, feature selection, or regularization)\")\n",
    "print(\"   - Scale/normalize numeric features\")\n",
    "print(\"   - Encode categorical variables appropriately\")\n",
    "print(\"   - Consider feature engineering based on domain knowledge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "w0x1y2z3-a4b5-c6d7-e8f9-g0h1i2j3k4l5",
   "metadata": {},
   "source": [
    "## 6. Save Analysis Results\n",
    "\n",
    "Let's save our analysis results for future reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "x1y2z3a4-b5c6-d7e8-f9g0-h1i2j3k4l5m6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the sample dataset\n",
    "df.to_csv('../data/sample_climbing_data.csv', index=False)\n",
    "print(\"Sample dataset saved to '../data/sample_climbing_data.csv'\")\n",
    "\n",
    "# Save reports\n",
    "with open('../data/exploration_report.txt', 'w') as f:\n",
    "    f.write(exploration_report)\n",
    "print(\"Data exploration report saved to '../data/exploration_report.txt'\")\n",
    "\n",
    "with open('../data/statistics_report.txt', 'w') as f:\n",
    "    f.write(stats_report)\n",
    "print(\"Statistical analysis report saved to '../data/statistics_report.txt'\")\n",
    "\n",
    "# Save correlation matrix\n",
    "corr_matrix.to_csv('../data/correlation_matrix.csv')\n",
    "print(\"Correlation matrix saved to '../data/correlation_matrix.csv'\")\n",
    "\n",
    "print(\"\\nAll analysis results have been saved successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}